import requests
import pandas as pd
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import nltk
from datetime import datetime, timedelta
import time
import json
from typing import List, Dict, Optional
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NewsSentimentAnalyzer:
    def __init__(self):
        """
        Initialise l'analyseur de sentiment d'actualit√©s
        """
        # T√©l√©charger les ressources NLTK n√©cessaires
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('vader_lexicon', quiet=True)
        except:
            logger.warning("Impossible de t√©l√©charger les ressources NLTK")
        
        # Initialiser VADER
        self.vader_analyzer = SentimentIntensityAnalyzer()
        
        # APIs gratuites disponibles
        self.news_apis = {
            'newsapi': {
                'url': 'https://newsapi.org/v2/everything',
                'key': None,  # √Ä remplacer par votre cl√© API
                'limit': 100  # Limite quotidienne gratuite
            },
            'gnews': {
                'url': 'https://gnews.io/api/v4/search',
                'key': None,  # √Ä remplacer par votre cl√© API
                'limit': 100
            }
        }
    
    def set_api_key(self, service: str, api_key: str):
        """
        Configure une cl√© API pour un service donn√©
        
        Args:
            service (str): 'newsapi' ou 'gnews'
            api_key (str): Cl√© API
        """
        if service in self.news_apis:
            self.news_apis[service]['key'] = api_key
            logger.info(f"Cl√© API configur√©e pour {service}")
        else:
            logger.error(f"Service {service} non reconnu")
    
    def fetch_news_newsapi(self, query: str = "france", language: str = "fr", 
                          days_back: int = 1) -> List[Dict]:
        """
        R√©cup√®re les actualit√©s via NewsAPI
        
        Args:
            query (str): Terme de recherche
            language (str): Langue des articles
            days_back (int): Nombre de jours en arri√®re
            
        Returns:
            List[Dict]: Liste des articles
        """
        if not self.news_apis['newsapi']['key']:
            logger.error("Cl√© API NewsAPI non configur√©e")
            return []
        
        from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
        
        params = {
            'q': query,
            'language': language,
            'from': from_date,
            'sortBy': 'publishedAt',
            'apiKey': self.news_apis['newsapi']['key'],
            'pageSize': 100
        }
        
        try:
            response = requests.get(self.news_apis['newsapi']['url'], params=params)
            response.raise_for_status()
            data = response.json()
            
            articles = []
            for article in data.get('articles', []):
                if article['title'] and article['description']:
                    articles.append({
                        'title': article['title'],
                        'description': article['description'],
                        'content': article.get('content', ''),
                        'url': article['url'],
                        'published_at': article['publishedAt'],
                        'source': article['source']['name']
                    })
            
            logger.info(f"R√©cup√©r√© {len(articles)} articles via NewsAPI")
            return articles
            
        except requests.RequestException as e:
            logger.error(f"Erreur lors de la r√©cup√©ration NewsAPI: {e}")
            return []
    
    def fetch_news_gnews(self, query: str = "actualit√©s france", language: str = "fr") -> List[Dict]:
        """
        R√©cup√®re les actualit√©s via GNews
        
        Args:
            query (str): Terme de recherche
            language (str): Langue des articles
            
        Returns:
            List[Dict]: Liste des articles
        """
        if not self.news_apis['gnews']['key']:
            logger.error("Cl√© API GNews non configur√©e")
            return []
        
        params = {
            'q': query,
            'lang': language,
            'country': 'fr',
            'max': 100,
            'apikey': self.news_apis['gnews']['key']
        }
        
        try:
            response = requests.get(self.news_apis['gnews']['url'], params=params)
            response.raise_for_status()
            data = response.json()
            
            articles = []
            for article in data.get('articles', []):
                articles.append({
                    'title': article['title'],
                    'description': article['description'],
                    'content': article.get('content', ''),
                    'url': article['url'],
                    'published_at': article['publishedAt'],
                    'source': article['source']['name']
                })
            
            logger.info(f"R√©cup√©r√© {len(articles)} articles via GNews")
            return articles
            
        except requests.RequestException as e:
            logger.error(f"Erreur lors de la r√©cup√©ration GNews: {e}")
            return []
    
    def fetch_news_rss(self, rss_urls: List[str]) -> List[Dict]:
        """
        R√©cup√®re les actualit√©s via RSS (gratuit, sans API)
        
        Args:
            rss_urls (List[str]): Liste des URLs RSS
            
        Returns:
            List[Dict]: Liste des articles
        """
        try:
            import feedparser
        except ImportError:
            logger.error("Installez feedparser: pip install feedparser")
            return []
        
        articles = []
        
        for rss_url in rss_urls:
            try:
                feed = feedparser.parse(rss_url)
                
                for entry in feed.entries[:20]:  # Limite √† 20 par flux
                    articles.append({
                        'title': entry.get('title', ''),
                        'description': entry.get('summary', ''),
                        'content': entry.get('content', [{}])[0].get('value', '') if entry.get('content') else '',
                        'url': entry.get('link', ''),
                        'published_at': entry.get('published', ''),
                        'source': feed.feed.get('title', 'RSS Feed')
                    })
                
                time.sleep(1)  # Respecter les serveurs RSS
                
            except Exception as e:
                logger.error(f"Erreur RSS pour {rss_url}: {e}")
        
        logger.info(f"R√©cup√©r√© {len(articles)} articles via RSS")
        return articles
    
    def analyze_sentiment_textblob(self, text: str) -> Dict:
        """
        Analyse le sentiment avec TextBlob
        
        Args:
            text (str): Texte √† analyser
            
        Returns:
            Dict: R√©sultats d'analyse
        """
        blob = TextBlob(text)
        polarity = blob.sentiment.polarity
        subjectivity = blob.sentiment.subjectivity
        
        # Classification du sentiment
        if polarity > 0.1:
            sentiment = 'positive'
        elif polarity < -0.1:
            sentiment = 'negative'
        else:
            sentiment = 'neutral'
        
        return {
            'method': 'TextBlob',
            'polarity': polarity,
            'subjectivity': subjectivity,
            'sentiment': sentiment,
            'confidence': abs(polarity)
        }
    
    def analyze_sentiment_vader(self, text: str) -> Dict:
        """
        Analyse le sentiment avec VADER
        
        Args:
            text (str): Texte √† analyser
            
        Returns:
            Dict: R√©sultats d'analyse
        """
        scores = self.vader_analyzer.polarity_scores(text)
        
        # D√©terminer le sentiment dominant
        if scores['compound'] >= 0.05:
            sentiment = 'positive'
        elif scores['compound'] <= -0.05:
            sentiment = 'negative'
        else:
            sentiment = 'neutral'
        
        return {
            'method': 'VADER',
            'compound': scores['compound'],
            'positive': scores['pos'],
            'negative': scores['neg'],
            'neutral': scores['neu'],
            'sentiment': sentiment,
            'confidence': abs(scores['compound'])
        }
    
    def analyze_article_sentiment(self, article: Dict) -> Dict:
        """
        Analyse le sentiment d'un article complet
        
        Args:
            article (Dict): Article √† analyser
            
        Returns:
            Dict: Article avec analyse de sentiment
        """
        # Combiner titre et description pour l'analyse
        full_text = f"{article.get('title', '')} {article.get('description', '')}"
        
        # Analyses avec les deux m√©thodes
        textblob_result = self.analyze_sentiment_textblob(full_text)
        vader_result = self.analyze_sentiment_vader(full_text)
        
        # Sentiment consensus
        sentiments = [textblob_result['sentiment'], vader_result['sentiment']]
        consensus_sentiment = max(set(sentiments), key=sentiments.count)
        
        # Confiance moyenne
        avg_confidence = (textblob_result['confidence'] + vader_result['confidence']) / 2
        
        # Ajouter les r√©sultats √† l'article
        article_with_sentiment = article.copy()
        article_with_sentiment.update({
            'textblob_analysis': textblob_result,
            'vader_analysis': vader_result,
            'consensus_sentiment': consensus_sentiment,
            'confidence_score': avg_confidence,
            'analysis_timestamp': datetime.now().isoformat()
        })
        
        return article_with_sentiment
    
    def process_news_batch(self, query: str = "actualit√©s", language: str = "fr", 
                          use_rss: bool = True) -> pd.DataFrame:
        """
        Traite un lot d'actualit√©s compl√®tes
        
        Args:
            query (str): Terme de recherche
            language (str): Langue
            use_rss (bool): Utiliser les flux RSS gratuits
            
        Returns:
            pd.DataFrame: DataFrame avec analyses de sentiment
        """
        all_articles = []
        
        # Flux RSS fran√ßais gratuits
        if use_rss:
            rss_feeds = [
                'https://www.lemonde.fr/rss/une.xml',
                'https://www.lefigaro.fr/rss/figaro_actualites.xml',
                'https://www.liberation.fr/arc/outboundfeeds/rss-all/',
                'https://www.francetvinfo.fr/titres.rss',
                'https://www.20minutes.fr/feeds/rss-actu.xml'
            ]
            rss_articles = self.fetch_news_rss(rss_feeds)
            all_articles.extend(rss_articles)
        
        # APIs payantes si configur√©es
        if self.news_apis['newsapi']['key']:
            newsapi_articles = self.fetch_news_newsapi(query, language)
            all_articles.extend(newsapi_articles)
        
        if self.news_apis['gnews']['key']:
            gnews_articles = self.fetch_news_gnews(query, language)
            all_articles.extend(gnews_articles)
        
        if not all_articles:
            logger.warning("Aucun article r√©cup√©r√©")
            return pd.DataFrame()
        
        # Supprimer les doublons par URL
        unique_articles = []
        seen_urls = set()
        for article in all_articles:
            if article['url'] not in seen_urls:
                unique_articles.append(article)
                seen_urls.add(article['url'])
        
        logger.info(f"Traitement de {len(unique_articles)} articles uniques")
        
        # Analyser le sentiment de chaque article
        analyzed_articles = []
        for i, article in enumerate(unique_articles):
            try:
                analyzed_article = self.analyze_article_sentiment(article)
                analyzed_articles.append(analyzed_article)
                
                if (i + 1) % 50 == 0:
                    logger.info(f"Analys√© {i + 1}/{len(unique_articles)} articles")
                    
            except Exception as e:
                logger.error(f"Erreur lors de l'analyse de l'article {i}: {e}")
        
        # Cr√©er DataFrame
        df = pd.DataFrame(analyzed_articles)
        
        if not df.empty:
            # Ajouter des colonnes pratiques pour le dashboard
            df['sentiment_score'] = df.apply(
                lambda row: row['vader_analysis']['compound'], axis=1
            )
            df['publication_date'] = pd.to_datetime(df['published_at'], errors='coerce')
            
            logger.info(f"Analyse termin√©e: {len(df)} articles trait√©s")
            
            # Statistiques rapides
            sentiment_counts = df['consensus_sentiment'].value_counts()
            logger.info(f"R√©partition des sentiments: {sentiment_counts.to_dict()}")
        
        return df
    
    def get_sentiment_summary(self, df: pd.DataFrame) -> Dict:
        """
        G√©n√®re un r√©sum√© des analyses de sentiment
        
        Args:
            df (pd.DataFrame): DataFrame avec les analyses
            
        Returns:
            Dict: R√©sum√© statistique
        """
        if df.empty:
            return {}
        
        summary = {
            'total_articles': len(df),
            'sentiment_distribution': df['consensus_sentiment'].value_counts().to_dict(),
            'average_sentiment_score': df['sentiment_score'].mean(),
            'most_positive_article': df.loc[df['sentiment_score'].idxmax()]['title'] if not df.empty else None,
            'most_negative_article': df.loc[df['sentiment_score'].idxmin()]['title'] if not df.empty else None,
            'sources_count': df['source'].nunique(),
            'analysis_date': datetime.now().isoformat()
        }
        
        return summary

# Exemple d'utilisation
if __name__ == "__main__":
    # Initialiser l'analyseur
    analyzer = NewsSentimentAnalyzer()
    
    # Optionnel: configurer les cl√©s API (payantes mais avec limites gratuites)
    # analyzer.set_api_key('newsapi', 'VOTRE_CLE_NEWSAPI')
    # analyzer.set_api_key('gnews', 'VOTRE_CLE_GNEWS')
    
    # Analyser les actualit√©s
    print("üîç R√©cup√©ration et analyse des actualit√©s...")
    df_news = analyzer.process_news_batch(
        query="France actualit√©s",
        language="fr",
        use_rss=True  # Utilise les flux RSS gratuits
    )
    
    if not df_news.empty:
        # Afficher le r√©sum√©
        summary = analyzer.get_sentiment_summary(df_news)
        print("\nüìä R√©sum√© de l'analyse:")
        print(f"Total d'articles: {summary['total_articles']}")
        print(f"R√©partition: {summary['sentiment_distribution']}")
        print(f"Score moyen: {summary['average_sentiment_score']:.2f}")
        
        # Sauvegarder les r√©sultats
        df_news.to_csv('news_sentiment_analysis.csv', index=False, encoding='utf-8')
        print("\nüíæ R√©sultats sauvegard√©s dans 'news_sentiment_analysis.csv'")
        
        # Afficher quelques exemples
        print("\nüì∞ Exemples d'articles analys√©s:")
        for _, row in df_news.head(3).iterrows():
            print(f"- {row['title'][:100]}...")
            print(f"  Sentiment: {row['consensus_sentiment']} (score: {row['sentiment_score']:.2f})")
            print(f"  Source: {row['source']}")
            print()
    else:
        print("‚ùå Aucun article r√©cup√©r√©")